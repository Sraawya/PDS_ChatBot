{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV data\n",
    "df = pd.read_csv('chatdata.csv')\n",
    "\n",
    "# Tokenizer and Model Configuration\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "config = GPT2Config.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel(config)\n",
    "\n",
    "# Define a simple dataset\n",
    "class ChatDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=1024):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user_input = self.data.iloc[idx]['question']\n",
    "        answer = self.data.iloc[idx]['answer']\n",
    "\n",
    "        # Combine user input and answer\n",
    "        conversation = f\"User: {user_input} Bot: {answer}\"\n",
    "\n",
    "        # Tokenize the conversation\n",
    "        input_ids = self.tokenizer.encode(conversation, max_length=self.max_length, return_tensors=\"pt\", truncation=True).squeeze()\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "        }\n",
    "\n",
    "# Prepare the dataset and dataloader\n",
    "dataset = ChatDataset(df, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Batch [100/228], Loss: 7.5707\n",
      "Epoch [1/5], Batch [200/228], Loss: 6.7739\n",
      "Epoch [1/5], Average Loss: 6.6468\n",
      "Epoch [2/5], Batch [100/228], Loss: 5.1584\n",
      "Epoch [2/5], Batch [200/228], Loss: 5.0258\n",
      "Epoch [2/5], Average Loss: 4.9782\n",
      "Epoch [3/5], Batch [100/228], Loss: 4.1895\n",
      "Epoch [3/5], Batch [200/228], Loss: 4.1715\n",
      "Epoch [3/5], Average Loss: 4.1514\n",
      "Epoch [4/5], Batch [100/228], Loss: 3.4828\n",
      "Epoch [4/5], Batch [200/228], Loss: 3.3306\n",
      "Epoch [4/5], Average Loss: 3.2638\n",
      "Epoch [5/5], Batch [100/228], Loss: 2.7568\n",
      "Epoch [5/5], Batch [200/228], Loss: 2.5100\n",
      "Epoch [5/5], Average Loss: 2.4848\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "num_epochs = 5\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop (fine-tuning)\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    for batch_num, batch in enumerate(dataloader, 1):\n",
    "        input_ids = batch['input_ids']\n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Print training progress\n",
    "        if batch_num % 100 == 0:\n",
    "            avg_loss = total_loss / batch_num\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Batch [{batch_num}/{num_batches}], Loss: {avg_loss:.4f}\", flush=True)\n",
    "\n",
    "    # Average loss for the epoch\n",
    "    avg_epoch_loss = total_loss / num_batches\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Average Loss: {avg_epoch_loss:.4f}\", flush=True)\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained('fine_tuned_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model\n",
    "model = GPT2LMHeadModel.from_pretrained('fine_tuned_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, tokenizer, user_input, df):\n",
    "    # Tokenize the user input\n",
    "    input_ids = tokenizer.encode(f\"User: {user_input} Bot:\", return_tensors=\"pt\", truncation=True)\n",
    "\n",
    "    # Generate a response using the fine-tuned GPT-2 model\n",
    "    output = model.generate(input_ids, max_length=100, num_beams=5, no_repeat_ngram_size=2, top_k=50, top_p=0.95, temperature=0.7)\n",
    "\n",
    "    # Decode the generated response\n",
    "    generated_response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Check if the generated response contains known keywords from the dataset\n",
    "    if any(keyword.lower() in generated_response.lower() for keyword in df['answer'].values):\n",
    "        return generated_response\n",
    "    else:\n",
    "        return \"I don't understand this. Please provide more information or contact the head of the department for assistance.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sraaw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sraaw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: how to pay online?\n",
      "Bot: I don't understand this. Please provide more information or contact the head of the department for assistance.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "user_input = \"how to pay online?\"\n",
    "response = test_model(model, tokenizer, user_input, df)\n",
    "print(\"User:\", user_input)\n",
    "print(\"Bot:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: how to pay online?\n",
      "Bot: User: how to pay online? Bot: No, students and faculty with UMKC webmail via https://outlook.office.com/mail16-235-1354. Forumkc.edu/: cashiers@edu. Center or cash payments. Contact payments in-286-Friday. The Cashiers Office at 816/235/out/@um: Access in person at 5 pm/C Wireless can be: Contact: Pay from your web pm65.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model = GPT2LMHeadModel.from_pretrained('fine_tuned_model')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "def test_model(model, tokenizer, user_input):\n",
    "    # Tokenize the user input\n",
    "    input_ids = tokenizer.encode(f\"User: {user_input} Bot:\", return_tensors=\"pt\", truncation=True)\n",
    "\n",
    "    # Generate a response using the fine-tuned GPT-2 model\n",
    "    output = model.generate(input_ids, max_length=100, num_beams=5, no_repeat_ngram_size=2, top_k=50, top_p=0.95, temperature=0.7)\n",
    "\n",
    "    # Decode the generated response\n",
    "    generated_response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    return generated_response\n",
    "\n",
    "# Example usage\n",
    "user_input = \"how to pay online?\"\n",
    "response = test_model(model, tokenizer, user_input)\n",
    "print(\"User:\", user_input)\n",
    "print(\"Bot:\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
